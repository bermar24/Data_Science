{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc1298e-e5c1-459a-a0d9-292e2fa5463b",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Using Classical & Neural Models\n",
    "\n",
    "## 1. Introduction\n",
    "This project studies sentiment classification on Twitter using the Kaggle \"Twitter Sentiment Dataset\" (Saurabh Shahane, 2021). The dataset contains cleaned tweets (`clean_text`) and sentiment labels in `category` with values -1 (negative), 0 (neutral), +1 (positive). The goal is to compare classical machine learning models against a text-oriented neural model (CNN–LSTM hybrid) and identify which approach is best for multiclass sentiment classification in terms of accuracy and robust F1 (macro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65845843-46e9-4f83-8f88-3c52777c39a7",
   "metadata": {},
   "source": [
    "## 2. Research Questions\n",
    "1. Which model achieves the best overall and per-class performance for predicting sentiment?\n",
    "2. How do TF-IDF and Word2Vec features compare when used with classical models?\n",
    "3. Does a CNN–LSTM hybrid outperform classical ensembles (Voting Classifier) on this dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00add81c-b28c-4aea-a122-8cc763d38606",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "- Source: Kaggle — Twitter Sentiment Dataset (Saurabh Shahane, 2021).\n",
    "- Columns: `clean_text` (string), `category` (int: -1, 0, 1).\n",
    "- Size: (162980, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a457ba-fe3d-4ad3-8436-1c261a47c66d",
   "metadata": {},
   "source": [
    "4. Methodology\n",
    "EDA & preprocessing: class distribution, text length, missing data handling, tokenization, stopword removal.\n",
    "Feature pipelines:\n",
    "TF-IDF vectorizer (for baseline classical models).\n",
    "Word2Vec embeddings (gensim): average tweet vectors for classical models.\n",
    "Keras Tokenizer + padding for NN (embedding layer + CNN–LSTM).\n",
    "Models:\n",
    "Baselines: Decision Tree, KNN, Logistic Regression.\n",
    "Ensemble: Voting Classifier (hard or soft) combining best performing classical models.\n",
    "Neural: CNN–LSTM hybrid (Embedding → Conv1D → MaxPool → LSTM → Dense).\n",
    "Hyperparameter tuning: GridSearchCV for classical models; manual / Keras Tuner for NN if time permits.\n",
    "Evaluation: accuracy, precision, recall, F1 (macro & per class), confusion matrix, ROC-AUC (one-vs-rest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f763b709-19b8-4296-888a-0d31e1272fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:02:27.019543: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-14 13:02:27.019755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-14 13:02:27.051496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-14 13:02:27.673831: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-14 13:02:27.674104: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# For Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# For neural model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Plotting utilities\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d813c-e2d1-45ad-aa74-c2e30ea319f2",
   "metadata": {},
   "source": [
    "## Arguments & Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8206861-a46e-43f4-a72f-31f0479391f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PLACEHOLDER = \"twitter_data.csv\"  # change if needed\n",
    "TARGET_COL = \"category\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268c4fa-db65-466a-ab0e-fc470044773c",
   "metadata": {},
   "source": [
    "## Load dataset & quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed8a7eb-0bca-4f43-bc80-c4c71d87a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (162980, 1)\n",
      "Columns: ['clean_text,category']\n",
      "                                 clean_text,category\n",
      "0  when modi promised “minimum government maximum...\n",
      "1  talk all the nonsense and continue all the dra...\n",
      "2  what did just say vote for modi  welcome bjp t...\n",
      "3  asking his supporters prefix chowkidar their n...\n",
      "4  answer who among these the most powerful world...\n",
      "\n",
      "--- Missing Values ---\n",
      "clean_text,category    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--data_path\", type=str, default=\"Twitter_Data.csv\")\n",
    "#     parser.add_argument(\"--output_dir\", type=str, default=\"outputs\")\n",
    "#     parser.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "#     parser.add_argument(\"--random_state\", type=int, default=42)\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "# os.makedirs(args.output_dir, exist_ok=True)\n",
    "# RND = args.random_state\n",
    "\n",
    "# df = pd.read_csv(args.data_path)\n",
    "\n",
    "def load_data(path=CSV_PLACEHOLDER):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found at {path}. Please place the dataset file there or change the path.\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=';')\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=';')\n",
    "    return df\n",
    "\n",
    "df = load_data(CSV_PLACEHOLDER)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Basic checks\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())\n",
    "# print(\"Class distribution:\\n\", df['category'].value_counts())\n",
    "\n",
    "# # Quick distribution plot\n",
    "# plt.figure(figsize=(6,4))\n",
    "# sns.countplot(x='category', data=df, order=[-1,0,1])\n",
    "# plt.title(\"Sentiment distribution\")\n",
    "# plt.savefig(os.path.join(args.output_dir, \"sentiment_distribution.png\"))\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fcad5-f694-449f-9f54-5c05191266ad",
   "metadata": {},
   "source": [
    "## 2. Preprocessing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16950256-0bf3-47f9-8035-7e251a7cfd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bermar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# def clean_text(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'http\\S+', '', text)      # remove URLs\n",
    "#     text = re.sub(r'@\\w+', '', text)         # remove mentions\n",
    "#     text = re.sub(r'[^a-z\\s]', '', text)     # remove non-letter chars\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     return text\n",
    "\n",
    "# df['clean_text'] = df['clean_text'].astype(str).apply(clean_text)\n",
    "\n",
    "# # Add basic features helpful for EDA\n",
    "# df['num_chars'] = df['clean_text'].apply(len)\n",
    "# df['num_words'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "# print(df[['num_chars','num_words']].describe())\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove headers, footers, and quotes (using parameters in fetch_20newsgroups is better for this specific dataset)\n",
    "    # However, for demonstration, let's include some regex cleaning for general text\n",
    "    text = re.sub(r'From:.*\\n', '', text) # Remove From line\n",
    "    text = re.sub(r'Subject:.*\\n', '', text) # Remove Subject line\n",
    "    text = re.sub(r'Organization:.*\\n', '', text) # Remove Organization line\n",
    "    text = re.sub(r'Lines:.*\\n', '', text) # Remove Lines line\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', text) # Remove email addresses\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\S+\\.com\\S*', '', text) # Remove .com URLs\n",
    "\n",
    "    text = text.lower() # Lowercasing\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "\n",
    "    return text\n",
    "\n",
    "# # Apply custom text cleaning (punctuation, lowercasing, numbers, etc.)\n",
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9880f-4a6b-4459-8b82-8a66aba4ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedb51f-ab78-4fd8-98d4-77740f5efec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70adb09-c7a8-4338-8d65-f5a2d54d6309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c30c4-03ca-47ec-86c5-c8a192c0a14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251971a-3819-49bb-955a-20a72fa6582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc27fdb-20d9-457c-a3fc-29a01ad30117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20802c88-d48b-4b1a-bb02-5b212d58b0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c223d4-3555-4a41-ba2c-23c4fc4d5958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbac37-d277-4956-be5e-51d49bdaa543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb9b8c-2adb-42e5-909c-c7b36f0d0ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python SentimentsKernel",
   "language": "python",
   "name": "sentimentskernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
